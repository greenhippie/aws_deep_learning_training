{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreaded Ensembling\n",
    "Two things are important here. Your time, and your results. Let's see if we can optimize for both! Use this notebook when you already have train, test, and validation data. Then you can train & tune a large number of models, and pull the results back in using an ensembling approach that takes the maximum prediction out of each classifier.\n",
    "\n",
    "Finally, you'll use SageMaker Search to find the best performing models from your bucket, and run multi-threaded batch transform jobs to run inference on all of your newly trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Upload your train and test data sets\n",
    "Make sure you have the label in the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', names = list(range(89)))\n",
    "test = pd.read_csv('test.csv', names = list(range(89)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train[0]).astype(\"float32\")\n",
    "train_features = np.array(train.drop(0, axis=1)).astype(\"float32\")\n",
    "test_labels = np.array(test[0]).astype(\"float32\")\n",
    "test_features  = np.array(test.drop(0, axis=1)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_estimator(clf, sess, role):\n",
    "\n",
    "    container = get_image_uri(boto3.Session().region_name, clf)\n",
    "\n",
    "    est = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, clf),\n",
    "                                    sagemaker_session=sess)\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(clf, sess, role):\n",
    "    \n",
    "    container = get_image_uri(boto3.Session().region_name, clf)\n",
    "\n",
    "    \n",
    "    if clf == 'xgboost':\n",
    "        est = get_base_estimator(clf, sess, role)\n",
    "        est.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "        \n",
    "    elif clf == 'linear-learner':\n",
    "        \n",
    "        est = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                               num_classes=2)\n",
    "\n",
    "    elif clf == 'knn':\n",
    "        est = sagemaker.KNN(role=sagemaker.get_execution_role(),\n",
    "                                              k = 10,\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='classifier',\n",
    "                                                sample_size = 200)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    elif clf == 'factorization-machines':\n",
    "        est = sagemaker.FactorizationMachines(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                                num_factors = 2)\n",
    "        \n",
    "        \n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_s3():\n",
    "    os.system('!aws s3 cp train.csv s3://ensemble-modeling/csv/train/train.csv')\n",
    "    os.system('!aws s3 cp test.csv s3://ensemble-modeling/csv/test/test.csv')\n",
    "    os.system('!aws s3 cp test.csv s3://ensemble-modeling/csv/validation/validation.csv')\n",
    "        \n",
    "copy_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuner(clf, est):\n",
    "        \n",
    "    if clf == 'xgboost':\n",
    "        objective_metric_name = 'validation:auc'\n",
    "\n",
    "        hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(1, 10)}\n",
    "        \n",
    "    elif clf == 'knn':\n",
    "        \n",
    "        objective_metric_name = 'test:accuracy'\n",
    "\n",
    "        hyperparameter_ranges = {'k': IntegerParameter(1, 1024),\n",
    "                        'sample_size': IntegerParameter(256, 20000000)}\n",
    "        \n",
    "    elif clf == 'linear-learner':\n",
    "        objective_metric_name = 'test:recall'\n",
    "        \n",
    "        hyperparameter_ranges = {'l1': ContinuousParameter(0.0000001,1),\n",
    "                            'use_bias': CategoricalParameter([True, False])}\n",
    "        \n",
    "    elif clf == 'factorization-machines':\n",
    "        objective_metric_name = 'test:binary_classification_accuracy'\n",
    "        \n",
    "        hyperparameter_ranges = {'bias_wd': IntegerParameter(1, 1000)}\n",
    "        \n",
    "    tuner = HyperparameterTuner(est,\n",
    "                    objective_metric_name,\n",
    "                    hyperparameter_ranges,\n",
    "                    max_jobs=30,\n",
    "                    max_parallel_jobs=3)\n",
    "    \n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_job(clf):\n",
    "\n",
    "    # build the estimator\n",
    "    est = get_estimator(clf, sess, role)\n",
    "\n",
    "    # get the hyperparameter tuner config \n",
    "    if clf == 'xgboost':\n",
    "        \n",
    "        tuner = get_tuner(clf, est)\n",
    "        \n",
    "        \n",
    "        tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n",
    "\n",
    "    else:\n",
    "        # set the records\n",
    "        train_records = est.record_set(train_features, train_labels, channel='train')\n",
    "        test_records = est.record_set(test_features, test_labels, channel='validation')\n",
    "\n",
    "        tuner = get_tuner(clf, est)\n",
    "        \n",
    "        tuner.fit([train_records, test_records])\n",
    "    \n",
    "    \n",
    "run_training_job('linear-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_loop(models_to_run):\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(run_training_job, models_to_run)\n",
    "    pool.close() \n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "client = boto3.client('sagemaker')\n",
    "bucket = 'ensemble-modeling'\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/train'.format(bucket), content_type='csv')\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/test/'.format(bucket), content_type='csv')\n",
    "\n",
    "# XGboost only likes a validation channel for hyperparameter tuning, not a test channel. So we'll set that up\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/validation/'.format(bucket), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the models you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clfs = ['xgboost', 'linear-learner', 'factorization-machines', 'knn']\n",
    "\n",
    "clfs = [ 'xgboost']\n",
    "\n",
    "magic_loop(clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select the best models\n",
    "Now, we're going to use SageMaker search to find the best performing models from the hyperparameter tuning jobs we just ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "smclient = boto3.client(service_name='sagemaker')\n",
    "\n",
    "# Search the training job by Amazon S3 location of model artifacts\n",
    "search_params={\n",
    "   \"MaxResults\": 100,\n",
    "   \"Resource\": \"TrainingJob\",\n",
    "   \"SearchExpression\": { \n",
    "      \"Filters\": [ \n",
    "         { \n",
    "            \"Name\": \"InputDataConfig.DataSource.S3DataSource.S3Uri\",\n",
    "            \"Operator\": \"Contains\",\n",
    "             \n",
    "             # set this to have a word that is in your bucket name\n",
    "            \"Value\": 'ensemble'\n",
    "         },\n",
    "        { \n",
    "            \"Name\": \"TrainingJobStatus\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": 'Completed'\n",
    "         }, \n",
    "    ],\n",
    "     \n",
    "   },\n",
    "    \n",
    "    \"SortBy\": \"Metrics.validation:auc\",\n",
    "    \"SortOrder\": \"Descending\"\n",
    "}\n",
    "results = smclient.search(**search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "def get_models(results):\n",
    "\n",
    "    role = sagemaker.get_execution_role()\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for each in results['Results']:\n",
    "\n",
    "        job_name = each['TrainingJob']['TrainingJobName']\n",
    "\n",
    "\n",
    "        artifact = each['TrainingJob']['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "        # get training image\n",
    "        image =  each['TrainingJob']['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "        m = Model(artifact, image, role = role, sagemaker_session = sess, name = job_name)\n",
    "\n",
    "        models.append(m)\n",
    "        \n",
    "    return models[:15]\n",
    "\n",
    "models = get_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ensemble Batch Transform\n",
    "Now, we're going to run a separate batch transform job for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('actual_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4118, 60)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_data.drop('0', axis=1)\n",
    "\n",
    "test_df.to_csv('test_data.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./test_data.csv to s3://ensemble-modeling/batch_test/test.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp test_data.csv s3://ensemble-modeling/batch_test/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: xgboost-190730-1958-024-b8a2fd71\n"
     ]
    }
   ],
   "source": [
    "def run_batch_transform(model):\n",
    "\n",
    "    transformer = model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m4.xlarge',\n",
    "        output_path='s3://ensemble-modeling/batch_results/{}'.format(model.name)\n",
    "    )\n",
    "\n",
    "    transformer.transform(data='s3://ensemble-modeling/batch_test/test.csv', content_type='text/csv')\n",
    "\n",
    "    \n",
    "for model in models:\n",
    "    run_batch_transform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = Pool(processes=multiprocessing.cpu_count())\n",
    "# transformed_rows = pool.map(run_batch_transform, models)\n",
    "# pool.close() \n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Consolidate batch results\n",
    "Finally, we'll pull together all of the batch job inferences. For each one, we'll take the maximum confidence level and consider that a positive prediction. Then we'll see how well that performs, relative to using a single XGBoost model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync s3://ensemble-modeling/batch_results/ /home/ec2-user/SageMaker/batch_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_data['0'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(y_true):\n",
    "    \n",
    "    frames  = []\n",
    "    \n",
    "    for sub_dir in os.listdir('/home/ec2-user/SageMaker/batch_results'):\n",
    "        if '.ipynb' not in sub_dir:\n",
    "\n",
    "            old_file = '/home/ec2-user/SageMaker/batch_results/{}/test.csv.out'.format(sub_dir)\n",
    "            \n",
    "            new_file = '/home/ec2-user/SageMaker/batch_results/{}/test.csv'.format(sub_dir)\n",
    "            \n",
    "            os.system('cp {} {}'.format( old_file, new_file))\n",
    "            \n",
    "            df = pd.read_csv('/home/ec2-user/SageMaker/batch_results/{}/test.csv'.format(sub_dir), names = [sub_dir])\n",
    "\n",
    "            frames.append(df)\n",
    "            \n",
    "    df = pd.concat(frames, axis=1)\n",
    "    \n",
    "    df['y_true'] = y_true\n",
    "            \n",
    "    return df\n",
    "        \n",
    "df = get_dataframe(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_results(df):\n",
    "\n",
    "    df['max'] = 0\n",
    "    df['min'] = 0\n",
    "    df['diff'] = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        top = max(row)\n",
    "        bottom = min(row)\n",
    "\n",
    "        diff = top - bottom\n",
    "\n",
    "\n",
    "        df.at[idx, 'max'] = top\n",
    "        df.at[idx, 'min'] = bottom\n",
    "        df.at[idx, 'diff'] = diff\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = consolidate_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgboost-190730-2044-022-2352eede</th>\n",
       "      <th>xgboost-190730-2044-004-739de6d8</th>\n",
       "      <th>xgboost-190730-2020-030-483f572c</th>\n",
       "      <th>xgboost-190730-2020-029-b7c0ecf1</th>\n",
       "      <th>xgboost-190730-2044-005-b614948b</th>\n",
       "      <th>xgboost-190730-2044-019-440eba24</th>\n",
       "      <th>xgboost-190730-2020-028-3a863835</th>\n",
       "      <th>xgboost-190730-1958-021-72e38862</th>\n",
       "      <th>xgboost-190730-2020-019-c5dba7b3</th>\n",
       "      <th>xgboost-190730-2044-021-949ec8ac</th>\n",
       "      <th>xgboost-190730-1958-024-b8a2fd71</th>\n",
       "      <th>xgboost-190730-1958-023-2e4c37ce</th>\n",
       "      <th>xgboost-190730-1958-015-b671b4dc</th>\n",
       "      <th>xgboost-190730-2020-026-b0f76816</th>\n",
       "      <th>xgboost-190730-1958-013-578ad74a</th>\n",
       "      <th>y_true</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100188</td>\n",
       "      <td>0.103718</td>\n",
       "      <td>0.104903</td>\n",
       "      <td>0.098098</td>\n",
       "      <td>0.092084</td>\n",
       "      <td>0.092530</td>\n",
       "      <td>0.097160</td>\n",
       "      <td>0.098769</td>\n",
       "      <td>0.090717</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>0.074131</td>\n",
       "      <td>0.094674</td>\n",
       "      <td>0.086106</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>0.089065</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.168533</td>\n",
       "      <td>0.176325</td>\n",
       "      <td>0.189338</td>\n",
       "      <td>0.208647</td>\n",
       "      <td>0.179116</td>\n",
       "      <td>0.182808</td>\n",
       "      <td>0.168847</td>\n",
       "      <td>0.195914</td>\n",
       "      <td>0.190306</td>\n",
       "      <td>0.171908</td>\n",
       "      <td>0.186766</td>\n",
       "      <td>0.141365</td>\n",
       "      <td>0.179298</td>\n",
       "      <td>0.210121</td>\n",
       "      <td>0.189402</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.282113</td>\n",
       "      <td>0.293109</td>\n",
       "      <td>0.251772</td>\n",
       "      <td>0.250157</td>\n",
       "      <td>0.254854</td>\n",
       "      <td>0.278672</td>\n",
       "      <td>0.256963</td>\n",
       "      <td>0.275336</td>\n",
       "      <td>0.270040</td>\n",
       "      <td>0.277161</td>\n",
       "      <td>0.251583</td>\n",
       "      <td>0.267924</td>\n",
       "      <td>0.222159</td>\n",
       "      <td>0.273822</td>\n",
       "      <td>0.242742</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040475</td>\n",
       "      <td>0.039072</td>\n",
       "      <td>0.035436</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.035632</td>\n",
       "      <td>0.038267</td>\n",
       "      <td>0.030731</td>\n",
       "      <td>0.028601</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>0.024260</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.030913</td>\n",
       "      <td>0.021417</td>\n",
       "      <td>0.029594</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039658</td>\n",
       "      <td>0.038584</td>\n",
       "      <td>0.031971</td>\n",
       "      <td>0.029252</td>\n",
       "      <td>0.032894</td>\n",
       "      <td>0.037440</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>0.040352</td>\n",
       "      <td>0.037843</td>\n",
       "      <td>0.040215</td>\n",
       "      <td>0.031937</td>\n",
       "      <td>0.031920</td>\n",
       "      <td>0.031521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   xgboost-190730-2044-022-2352eede  xgboost-190730-2044-004-739de6d8  \\\n",
       "0                          0.100188                          0.103718   \n",
       "1                          0.168533                          0.176325   \n",
       "2                          0.282113                          0.293109   \n",
       "3                          0.040475                          0.039072   \n",
       "4                          0.039658                          0.038584   \n",
       "\n",
       "   xgboost-190730-2020-030-483f572c  xgboost-190730-2020-029-b7c0ecf1  \\\n",
       "0                          0.104903                          0.098098   \n",
       "1                          0.189338                          0.208647   \n",
       "2                          0.251772                          0.250157   \n",
       "3                          0.035436                          0.026864   \n",
       "4                          0.031971                          0.029252   \n",
       "\n",
       "   xgboost-190730-2044-005-b614948b  xgboost-190730-2044-019-440eba24  \\\n",
       "0                          0.092084                          0.092530   \n",
       "1                          0.179116                          0.182808   \n",
       "2                          0.254854                          0.278672   \n",
       "3                          0.035632                          0.038267   \n",
       "4                          0.032894                          0.037440   \n",
       "\n",
       "   xgboost-190730-2020-028-3a863835  xgboost-190730-1958-021-72e38862  \\\n",
       "0                          0.097160                          0.098769   \n",
       "1                          0.168847                          0.195914   \n",
       "2                          0.256963                          0.275336   \n",
       "3                          0.030731                          0.028601   \n",
       "4                          0.033342                          0.028855   \n",
       "\n",
       "   xgboost-190730-2020-019-c5dba7b3  xgboost-190730-2044-021-949ec8ac  \\\n",
       "0                          0.090717                          0.098029   \n",
       "1                          0.190306                          0.171908   \n",
       "2                          0.270040                          0.277161   \n",
       "3                          0.036439                          0.040877   \n",
       "4                          0.035477                          0.040352   \n",
       "\n",
       "   xgboost-190730-1958-024-b8a2fd71  xgboost-190730-1958-023-2e4c37ce  \\\n",
       "0                          0.074131                          0.094674   \n",
       "1                          0.186766                          0.141365   \n",
       "2                          0.251583                          0.267924   \n",
       "3                          0.024260                          0.031481   \n",
       "4                          0.037843                          0.040215   \n",
       "\n",
       "   xgboost-190730-1958-015-b671b4dc  xgboost-190730-2020-026-b0f76816  \\\n",
       "0                          0.086106                          0.108009   \n",
       "1                          0.179298                          0.210121   \n",
       "2                          0.222159                          0.273822   \n",
       "3                          0.030913                          0.021417   \n",
       "4                          0.031937                          0.031920   \n",
       "\n",
       "   xgboost-190730-1958-013-578ad74a  y_true  max  min  diff  \n",
       "0                          0.089065       1    1    0     1  \n",
       "1                          0.189402       1    1    0     1  \n",
       "2                          0.242742       1    1    0     1  \n",
       "3                          0.029594       0    0    0     0  \n",
       "4                          0.031521       0    0    0     0  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate Confusion Matrix\n",
    "At the end, let's chart a plot for the performance of each of these models. Did the ensembling help? Which model appears to be the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results without ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 19.46%, Recall = 70.15%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3595</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions   0.0  1.0\n",
       "actuals               \n",
       "0            3595   40\n",
       "1             389   94"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_confusion_matrix(df, model_column):\n",
    "    \n",
    "    mx = pd.crosstab(index=df['y_true'], columns=np.round(df[model_column]), rownames=['actuals'], colnames=['predictions'])\n",
    "\n",
    "    # lower right corner\n",
    "    tps = mx.iloc[1, 1]\n",
    "        \n",
    "    # upper right corner\n",
    "    fps = mx.iloc[0, 1]\n",
    "    \n",
    "    # lower left corner\n",
    "    fns = mx.iloc[1, 0]\n",
    "    \n",
    "    precision = np.round(tps / (tps + fns), 4) * 100\n",
    "    \n",
    "    recall = np.round(tps / (tps + fps), 4) * 100\n",
    "    \n",
    "    print ('Precision = {}%, Recall = {}%'.format(precision, recall))\n",
    "    \n",
    "    return mx\n",
    "\n",
    "get_confusion_matrix(df,'xgboost-190730-2044-022-2352eede')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results With Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 100.0%, Recall = 100.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions     0    1\n",
       "actuals               \n",
       "0            3635    0\n",
       "1               0  483"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_confusion_matrix(df, 'max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! In this small example, we were able to see remarkable performance, simply by dramatically increasing the number of models we used to make inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
